# -*- coding: utf-8 -*-
"""Neural Network Polynomial Features.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FaEyUus-H0ABsncVzfj4R2YlwAC98qSr
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

plt.style.use('ggplot')

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error, max_error, make_scorer
from scipy.stats import pearsonr
from hydroeval import  kge, nse
import tpot
from tpot import TPOTRegressor
from sklearn.model_selection import RepeatedKFold



#%%----------------------------------------------------------------------------   
pd.options.display.float_format = '{:.3f}'.format
from read_data_ankara import *
basename='pfann_ankara__'

datasets = [
            #read_data_ankara(variation= 3,station='Ankara', test=0.25, expand_features=False, ),
            read_data_ankara(variation= 6,station='Ankara', test=0.25, expand_features=False, ),
            #read_data_ankara(variation=12,station='Ankara', test=0.25, expand_features=False, ),
           ]     
#print('tpot: %s' % tpot.__version__)

plot=True
n_runs=10
for run in range(0, n_runs):
    random_seed=run+10
    
    for dataset in datasets:
        dr=dataset['name'].replace(' ','_').replace("'","").lower()
        path='./pkl_'+basename+'_'+dr+'/'
        os.system('mkdir '+path.replace("-","_").lower())
        
        for tk, tn in enumerate(dataset['target_names']):
            #print (tk, tn)
            dataset_name = dataset['name']
            target                          = dataset['target_names'][tk]
            y_train, y_test                 = dataset['y_train'][tk], dataset['y_test'][tk]
            dataset_name, X_train, X_test   = dataset['name'], dataset['X_train'], dataset['X_test']
            n_samples_train, n_features     = dataset['n_samples'], dataset['n_features']
            task, normalize                 = dataset['task'], dataset['normalize']
            n_samples_test                  = len(y_test)
            
            s=''+'\n'
            s+='='*80+'\n'
            s+='Dataset                    : '+dataset_name+' -- '+target+'\n'
            s+='Number of training samples : '+str(n_samples_train) +'\n'
            s+='Number of testing  samples : '+str(n_samples_test) +'\n'
            s+='Number of features         : '+str(n_features)+'\n'
            s+='Normalization              : '+str(normalize)+'\n'
            s+='Task                       : '+str(dataset['task'])+'\n'
            s+='Reference                  : '+str(dataset['reference'])+'\n'
            s+='='*80
            s+='\n'            
            
            shape=(X_train.shape[1],)
            


            # define model evaluation
            cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=None)
            # define search
            model = TPOTRegressor(generations=5, population_size=50, cv=cv, scoring='neg_mean_squared_error', verbosity=2, random_state=None, n_jobs=-1)
            # perform the search
            model.fit(X_train, y_train)
            # export the best model
            model.export('tpot_best_model.py')

            #%%
            y_pred = model.predict(X_test)
            y_pred = np.array(y_pred)
            rmse_, r2 = mean_squared_error(y_test, y_pred)**.5, r2_score(y_test, y_pred)
            r=pearsonr(y_test.ravel(), y_pred.ravel())[0] 
            kge_=kge(y_test.ravel(), y_pred.ravel())[0][0]
            nse_=nse(y_test.ravel(), y_pred.ravel())
            #print(rmse_, r2,r, nse_, kge_)
                          
            pl.rc('text', usetex=True)
            pl.rc('font', family='serif',  serif='Times')
            if plot:
                fig = pl.figure(figsize=[10,4])
                pl.plot(y_test, 'r-o', y_pred,'b-.o', ms=4); pl.legend(['Observed', 'Predicted'])
                pl.title(dataset_name+' - '+'\n'+'RMSE = '+str(rmse_)+'\n'+'R$^2$ = '+str(r2)+'\n'+'KGE = '+str(kge_))
                pl.show()
            #
            #fig = pl.figure(figsize=[8,8])
            #pl.plot(y_test,y_test, 'r-', y_test,y_pred,'bo')
            #pl.title(dataset_name+' - '+samples+' - '+strategy+'\n'+'RMSE = '+str(rmse_)+'\n'+'NSE = '+str(nse_)+'\n'+'KGE = '+str(kge_))
            #pl.show()
            #s1 = "%3d: "%run+dataset_name.ljust(15)+' - '+"%0.3f"%rmse_+' - '+"%0.3f"%nse_
            #s1+= ' >> '+"%0.3f"%beta
            #s1+= ' | '+ ', '.join(feature_names[ft])+' -- '
            #s1+= ' '.join(["%1.6f"%i for i in model.coef_])+" | %1.3f"%model.intercept_
            #s1+= ' '.join(["%1.6f"%i for i in model[model.steps[-1][0]].coef_])+" | %1.3f"%model[model.steps[-1][0]].intercept_
            #print(s1)
# #%%
#                 l={
#                 'Y_TRAIN_TRUE':y_train, 'Y_TRAIN_PRED':model.predict(X_train[:,ft]), 
#                 'Y_TEST_TRUE':y_test, 'Y_TEST_PRED':y_pred, 'RUN':run,            
#                 'EST_PARAMS':{'l1_ratio':z[-1], 'alpha':z[-2]}, 
#                 'PARAMS':z, 'ESTIMATOR':model, 'FEATURE_NAMES':feature_names,
#                 'SEED':random_seed, 'DATASET_NAME':dataset_name,
#                 'ALGO':'DE', 'ALGO_STRATEGY':strategy,
#                 'ACTIVE_VAR':ft, 'ACTIVE_VAR_NAMES':feature_names[ft],
#                 'MODEL_COEF':model.coef_, 'MODEL_INTERCEPT':model.intercept_,
#                  #'MODEL_COEF':model[model.steps[-1][0]].coef_, 'MODEL_INTERCEPT':model[model.steps[-1][0]].intercept_,
#                 'BETA':beta,
#                 }
                
#                 pk=(path+basename+'_'+("%15s"% dataset_name).rjust(15)+
#                     '_run_'+str("{:02d}".format(run))+'_'+
#                     '_'+samples+'_'+
#                     '_'+'beta'+str("%1.2f"%beta).replace('.','p')+'_'+
#                     ("%15s"%target).rjust(15)+'.pkl')
#                 pk=pk.replace(' ','_').replace("'","").lower()
#                 pk=pk.replace('(','_').replace(")","_").lower()
#                 pk=pk.replace('[','_').replace("]","_").lower()
#                 pk=pk.replace('-','_').replace("-","_").lower()
#                 pd.DataFrame([l]).to_pickle(pk)
#%%

