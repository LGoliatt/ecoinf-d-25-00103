# -*- coding: utf-8 -*-
"""Neural Network Polynomial Features.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FaEyUus-H0ABsncVzfj4R2YlwAC98qSr
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

plt.style.use('ggplot')

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error, max_error, make_scorer
from scipy.stats import pearsonr
from hydroeval import  kge, nse


# iris = load_iris()
# X = iris['data']
# y = iris['target']
# names = iris['target_names']
# feature_names = iris['feature_names']

# from sklearn.decomposition import PCA

# X2D = PCA().fit_transform(X)

# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
# for target, target_name in enumerate(names):
#     X_plot = X2D[y == target]
#     ax1.plot(X_plot[:, 0], X_plot[:, 1], 
#              linestyle='none', 
#              marker='o', 
#              label=target_name)
# ax1.set_xlabel(feature_names[0])
# ax1.set_ylabel(feature_names[1])
# ax1.axis('equal')
# ax1.legend();

# for target, target_name in enumerate(names):
#     X_plot = X2D[y == target]
#     ax2.plot(X_plot[:, 0], X_plot[:, 1], 
#              linestyle='none', 
#              marker='o', 
#              label=target_name)
# ax2.set_xlabel(feature_names[2])
# ax2.set_ylabel(feature_names[3])
# ax2.axis('equal')
# ax2.legend();

# # Scale data to have mean 0 and variance 1 
# # which is importance for convergence of the neural network
# scaler = StandardScaler()
# X_scaled = scaler.fit_transform(X)

# # Split the data set into training and testing
# X_train, X_test, y_train, y_test = train_test_split(
#     X_scaled, y, test_size=0.3, random_state=None)

# #X_train, X_val, y_train, y_val = train_test_split(
# #    X_train, y_train, test_size=0.2, random_state=None)

import torch
import torch.nn.functional as F
import torch.nn as nn
from torch.autograd import Variable

class ANN(nn.Module):
    def __init__(self, input_dim):
        super(ANN, self).__init__()
        self.layer1 = nn.Linear(input_dim, 50)
        self.layer2 = nn.Linear(50, 50)
        self.layer3 = nn.Linear(50, 1)
        
    def forward(self, x):
        x = F.relu(self.layer1(x))
        x = F.relu(self.layer2(x))
        x = F.leaky_relu(self.layer3(x), negative_slope=1,)
        return x


import torch
from sklearn.preprocessing import PolynomialFeatures

class PolynomialFeaturesTorch(torch.nn.Module):
    def __init__(self, degree=2, interaction_only=False, include_bias=False):
        super(PolynomialFeaturesTorch, self).__init__()
        self.poly = PolynomialFeatures(
            degree = degree, 
            interaction_only = interaction_only,
            include_bias = include_bias
        )

    def forward(self, x):
        out = torch.from_numpy(
            self.poly.fit_transform(x.detach().numpy())
            )
        return out

class PFANN(nn.Module):
    def __init__(self, input_dim):
        super(PFANN, self).__init__()
        self.poly = PolynomialFeaturesTorch()
        
        self.n_input  = int((input_dim+1)*(input_dim+2)/2 -1)

        self.layer1 = nn.Linear(self.n_input, 50)
        self.layer2 = nn.Linear(50, 50)
        self.layer3 = nn.Linear(50, 1)
        
    def forward(self, x):

        x = self.poly(x)
        x = F.relu(self.layer1(x))      
        x = F.relu(self.layer2(x))      
        x = F.relu(self.layer3(x))
      
        return x



class MultipleRegression(nn.Module):
    def __init__(self, num_features):
        super(MultipleRegression, self).__init__()
        
        self.layer_1 = nn.Linear(num_features, 16)
        self.layer_2 = nn.Linear(16, 32)
        self.layer_3 = nn.Linear(32, 16)
        self.layer_out = nn.Linear(16, 1)
        
        self.relu = nn.ReLU()
    def forward(self, inputs):
            x = self.relu(self.layer_1(inputs))
            x = self.relu(self.layer_2(x))
            x = self.relu(self.layer_3(x))
            x = self.layer_out(x)
            return (x)
    def predict(self, test_inputs):
            x = self.relu(self.layer_1(test_inputs))
            x = self.relu(self.layer_2(x))
            x = self.relu(self.layer_3(x))
            x = self.layer_out(x)
            return (x)




model_list=[
        #ANN,
        #PFANN,
        MultipleRegression,
    ]

#%%
# for Model in model_list:
        

#     import tqdm
    
#     EPOCHS  = 100
#     X_train_ = Variable(torch.from_numpy(X_train)).float()
#     y_train_ = Variable(torch.from_numpy(y_train)).long()
#     #X_val_  = Variable(torch.from_numpy(X_val)).float()
#     #y_val_  = Variable(torch.from_numpy(y_val)).long()
#     X_test_  = Variable(torch.from_numpy(X_test)).float()
#     y_test_  = Variable(torch.from_numpy(y_test)).long()

#     model     = Model(X_train_.shape[1])
#     optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
#     loss_fn   = nn.L1Loss()
    

#     loss_list     = np.zeros((EPOCHS,))
#     accuracy_list = np.zeros((EPOCHS,))
    
#     for epoch in tqdm.trange(EPOCHS):
#         y_pred = model(X_train_)
#         loss = loss_fn(y_pred, y_train_)
#         loss_list[epoch] = loss.item()
        
#         # Zero gradients
#         optimizer.zero_grad()
#         loss.backward()
#         optimizer.step()
        
#         #with torch.no_grad():
#         #    y_val_pred = model(X_val_)
#         #    correct = (torch.argmax(y_val_pred, dim=1) == y_val_).type(torch.FloatTensor)
#         #    accuracy_list[epoch] = correct.mean()
    
#     #fig, (ax1, ax2) = plt.subplots(2, figsize=(12, 6), sharex=True)
    
#     #ax1.plot(accuracy_list)
#     #ax1.set_ylabel("validation accuracy")
#     #ax2.plot(loss_list)
#     #ax2.set_ylabel("validation loss")
#     #ax2.set_xlabel("epochs");
    
    
#     y_test_pred = model(X_test_)
    
#     cm=confusion_matrix( y_test_pred.argmax(axis=1), y_test_ )
#     print()
#     print(cm)

criterion = nn.MSELoss()

#%%



#%%----------------------------------------------------------------------------   
pd.options.display.float_format = '{:.3f}'.format
from read_data_ankara import *
basename='pfann_ankara__'

datasets = [
            #read_data_ankara(variation= 3,station='Ankara', test=0.25, expand_features=False, ),
            read_data_ankara(variation= 6,station='Ankara', test=0.25, expand_features=False, ),
            #read_data_ankara(variation=12,station='Ankara', test=0.25, expand_features=False, ),
           ]     

plot=True
n_runs=10
for run in range(0, n_runs):
    random_seed=run+10
    
    for dataset in datasets:
        dr=dataset['name'].replace(' ','_').replace("'","").lower()
        path='./pkl_'+basename+'_'+dr+'/'
        os.system('mkdir '+path.replace("-","_").lower())
        
        for tk, tn in enumerate(dataset['target_names']):
            #print (tk, tn)
            dataset_name = dataset['name']
            target                          = dataset['target_names'][tk]
            y_train, y_test                 = dataset['y_train'][tk], dataset['y_test'][tk]
            dataset_name, X_train, X_test   = dataset['name'], dataset['X_train'], dataset['X_test']
            n_samples_train, n_features     = dataset['n_samples'], dataset['n_features']
            task, normalize                 = dataset['task'], dataset['normalize']
            n_samples_test                  = len(y_test)
            
            s=''+'\n'
            s+='='*80+'\n'
            s+='Dataset                    : '+dataset_name+' -- '+target+'\n'
            s+='Number of training samples : '+str(n_samples_train) +'\n'
            s+='Number of testing  samples : '+str(n_samples_test) +'\n'
            s+='Number of features         : '+str(n_features)+'\n'
            s+='Normalization              : '+str(normalize)+'\n'
            s+='Task                       : '+str(dataset['task'])+'\n'
            s+='Reference                  : '+str(dataset['reference'])+'\n'
            s+='='*80
            s+='\n'            
            
            shape=(X_train.shape[1],)

            for Model in model_list:
                    
            
                import tqdm
                
                EPOCHS  = 1000
                X_train_ = Variable(torch.from_numpy(X_train)).float()
                y_train_ = Variable(torch.from_numpy(y_train)).float()
                #X_val_  = Variable(torch.from_numpy(X_val)).float()
                #y_val_  = Variable(torch.from_numpy(y_val)).float()
                X_test_  = Variable(torch.from_numpy(X_test)).float()
                y_test_  = Variable(torch.from_numpy(y_test)).float()
            
                # model     = Model(X_train_.shape[1])
                # optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
                # loss_fn   = nn.MSELoss()
                
            
                # loss_list     = np.zeros((EPOCHS,))
                # accuracy_list = np.zeros((EPOCHS,))
            
                
                # for epoch in tqdm.trange(EPOCHS):
                #     y_train_pred = model(X_train_)
                #     loss = loss_fn(y_train_pred, y_train_)
                #     loss_list[epoch] = loss.item()
                    
                #     # Zero gradients
                #     optimizer.zero_grad()
                #     loss.backward()
                #     optimizer.step()


                # y_test_pred = model(X_test_)                    
                # pl.plot(loss_list)
                # pl.show()
                
                model     = Model(X_train_.shape[1])
                device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
                #model.to(device)
                print(model)
                                
                criterion = nn.MSELoss()
                optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
                
                loss_stats = {
                    'train': [],
                    "val": []
                }
                        
                for epoch in tqdm.trange(EPOCHS):
                      y_train_pred = model(X_train_)
                      loss = criterion(y_train_pred, y_train_)
                      #loss_list[epoch] = loss.item()
                     
                      # Zero gradients
                      optimizer.zero_grad()
                      loss.backward()
                      optimizer.step()        
                   
            #%%
                y_test_pred = model.predict(X_test_)
                y_pred = y_test_pred.detach().numpy().ravel()
                y_pred = np.array(y_pred)
                rmse_, r2 = mean_squared_error(y_test, y_pred)**.5, r2_score(y_test, y_pred)
                r=pearsonr(y_test.ravel(), y_pred.ravel())[0] 
                kge_=kge(y_test.ravel(), y_pred.ravel())[0][0]
                nse_=nse(y_test.ravel(), y_pred.ravel())
                #print(rmse_, r2,r, nse_, kge_)
                              
                pl.rc('text', usetex=True)
                pl.rc('font', family='serif',  serif='Times')
                if plot:
                    fig = pl.figure(figsize=[10,4])
                    pl.plot(y_test, 'r-o', y_pred,'b-.o', ms=4); pl.legend(['Observed', 'Predicted'])
                    pl.title(dataset_name+' - '+'\n'+'RMSE = '+str(rmse_)+'\n'+'R$^2$ = '+str(r2)+'\n'+'KGE = '+str(kge_))
                    pl.show()
                #
                #fig = pl.figure(figsize=[8,8])
                #pl.plot(y_test,y_test, 'r-', y_test,y_pred,'bo')
                #pl.title(dataset_name+' - '+samples+' - '+strategy+'\n'+'RMSE = '+str(rmse_)+'\n'+'NSE = '+str(nse_)+'\n'+'KGE = '+str(kge_))
                #pl.show()
                #s1 = "%3d: "%run+dataset_name.ljust(15)+' - '+"%0.3f"%rmse_+' - '+"%0.3f"%nse_
                #s1+= ' >> '+"%0.3f"%beta
                #s1+= ' | '+ ', '.join(feature_names[ft])+' -- '
                #s1+= ' '.join(["%1.6f"%i for i in model.coef_])+" | %1.3f"%model.intercept_
                #s1+= ' '.join(["%1.6f"%i for i in model[model.steps[-1][0]].coef_])+" | %1.3f"%model[model.steps[-1][0]].intercept_
                #print(s1)
    # #%%
    #                 l={
    #                 'Y_TRAIN_TRUE':y_train, 'Y_TRAIN_PRED':model.predict(X_train[:,ft]), 
    #                 'Y_TEST_TRUE':y_test, 'Y_TEST_PRED':y_pred, 'RUN':run,            
    #                 'EST_PARAMS':{'l1_ratio':z[-1], 'alpha':z[-2]}, 
    #                 'PARAMS':z, 'ESTIMATOR':model, 'FEATURE_NAMES':feature_names,
    #                 'SEED':random_seed, 'DATASET_NAME':dataset_name,
    #                 'ALGO':'DE', 'ALGO_STRATEGY':strategy,
    #                 'ACTIVE_VAR':ft, 'ACTIVE_VAR_NAMES':feature_names[ft],
    #                 'MODEL_COEF':model.coef_, 'MODEL_INTERCEPT':model.intercept_,
    #                  #'MODEL_COEF':model[model.steps[-1][0]].coef_, 'MODEL_INTERCEPT':model[model.steps[-1][0]].intercept_,
    #                 'BETA':beta,
    #                 }
                    
    #                 pk=(path+basename+'_'+("%15s"% dataset_name).rjust(15)+
    #                     '_run_'+str("{:02d}".format(run))+'_'+
    #                     '_'+samples+'_'+
    #                     '_'+'beta'+str("%1.2f"%beta).replace('.','p')+'_'+
    #                     ("%15s"%target).rjust(15)+'.pkl')
    #                 pk=pk.replace(' ','_').replace("'","").lower()
    #                 pk=pk.replace('(','_').replace(")","_").lower()
    #                 pk=pk.replace('[','_').replace("]","_").lower()
    #                 pk=pk.replace('-','_').replace("-","_").lower()
    #                 pd.DataFrame([l]).to_pickle(pk)
#%%

